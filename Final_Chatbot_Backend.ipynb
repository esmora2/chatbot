{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROMOA_AVU4My",
        "outputId": "ebaf39f5-00f3-4d57-b52b-56b969e2c6d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh # download ollama api\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Create a Python script to start the Ollama API server in a separate thread\n",
        "\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def ollama():\n",
        "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "ollama_thread = threading.Thread(target=ollama)\n",
        "ollama_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gkw3spsxVLpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3736dbf3-198b-4e00-a96d-6a031f3e759b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "!ollama pull llama3.1:8b\n",
        "#clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVtt54xhVvkc",
        "outputId": "484a875c-3019-4fe1-fbc0-b9fb2e1057e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightrag[ollama] in /usr/local/lib/python3.10/dist-packages (0.1.0b6)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (2.2.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (3.1.4)\n",
            "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (4.0.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.26.4)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.0.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (4.66.5)\n",
            "Requirement already satisfied: ollama<0.3.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (0.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (2.1.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (24.2.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U lightrag[ollama]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drecxk3fkLfm",
        "outputId": "fee78e1a-0425-4156-a802-a4e0dde4c178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightrag[ollama] in /usr/local/lib/python3.10/dist-packages (0.1.0b6)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (2.2.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (3.1.4)\n",
            "Requirement already satisfied: jsonlines<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (4.0.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.26.4)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (1.0.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (6.0.2)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (4.66.5)\n",
            "Requirement already satisfied: ollama<0.3.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lightrag[ollama]) (0.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.3->lightrag[ollama]) (2.1.5)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines<5.0.0,>=4.0.0->lightrag[ollama]) (24.2.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->lightrag[ollama]) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<0.3.0,>=0.2.1->lightrag[ollama]) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U lightrag[ollama]\n",
        "\n",
        "from lightrag.core.generator import Generator\n",
        "from lightrag.core.component import Component\n",
        "from lightrag.core.model_client import ModelClient\n",
        "from lightrag.components.model_client import OllamaClient, GroqAPIClient\n",
        "\n",
        "import time\n",
        "\n",
        "from lightrag.components.model_client import OllamaClient\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YziM-F3Zw6E",
        "outputId": "886ed370-6a3d-42d8-9ca4-443add4fd557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar datos del CSV\n",
        "df = pd.read_csv('basecsvf.csv')\n",
        "\n",
        "df['Respuesta'] = df['Respuesta'].apply(lambda x: str(x) if pd.notnull(x) else '')\n",
        "df['Pregunta'] = df['Pregunta'].apply(lambda x: str(x) if pd.notnull(x) else '')\n",
        "\n",
        "def normalize(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['Pregunta'] = df['Pregunta']\n",
        "df['Respuesta'] = df['Respuesta']\n",
        "\n",
        "# Concatenar preguntas y respuestas\n",
        "df['PreguntaRespuesta'] = df['Pregunta'] + \" [SEP] \" + df['Respuesta']\n",
        "\n",
        "# Obtener la lista de textos concatenados\n",
        "preguntas_respuestas = df['PreguntaRespuesta'].tolist()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "\n",
        "embeddings = get_embeddings(preguntas_respuestas)\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "d = 768\n",
        "\n",
        "vectors = np.array([embedding.numpy() for embedding in embeddings], dtype='float32')\n",
        "\n",
        "index = faiss.IndexFlatL2(d)\n",
        "\n",
        "index.add(vectors)\n",
        "\n",
        "faiss.write_index(index, 'mi_indice_faiss.index')\n",
        "\n",
        "index = faiss.read_index('mi_indice_faiss.index')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "nueva_pregunta = \"Hola, quién eres?\"\n",
        "\n",
        "nueva_embedding = get_embeddings([nueva_pregunta])[0].numpy().astype('float32')\n",
        "\n",
        "distances, indices = index.search(np.array([nueva_embedding]), k=3)\n",
        "\n",
        "similarity_percentage = 100 * (1 - distances[0][0])\n",
        "print(f\"Similitud: {similarity_percentage}%\")\n",
        "\n",
        "max_index = len(preguntas_respuestas) - 1\n",
        "valid_indices = [i for i in indices[0] if i <= max_index]\n",
        "respuestas_cercanas = [preguntas_respuestas[i] for i in valid_indices]\n",
        "\n",
        "base_datos_vectorial = \"\\n\".join(respuestas_cercanas)\n",
        "\n",
        "print(base_datos_vectorial)\n",
        "base_datos_vectorial = respuestas_cercanas\n",
        "respuestas_cercanas"
      ],
      "metadata": {
        "id": "ugbueKwMx-uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a8a649-3ee6-4bf1-84f2-e77f80eff10e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similitud: -803.6664009094238%\n",
            "¿En qué me puede ayudar bienestar estudiantil? [SEP] En el artículo 86 indica la s funciones de Unidad de Bienestar Estudiantil.- \"Las instituciones de educación superior mantendrán una unidad administrativa de Bienestar Estudiantil destinada a promover la orientación vocacional y profesional, facilitar la obtención de créditos, estímulos, ayudas económicas y becas, y ofrecer los servicios asistenciales que se determinen en las normativas de cada institución. Esta unidad,\n",
            "además, se encargará de promover un ambiente de respeto a los derechos y a la integridad física, psicológica y sexual de las y los estudiantes, en un ambiente libre de violencia, y brindará asistencia a quienes demanden por violaciones de estos derechos.\n",
            "¿Por qué razón se me anularon mis materias? [SEP] 1. Por una mala gestión de las personas encargas de las aulas virtuales (Utic ) \n",
            "2 . No se cancelaron los pagos correspondientes a perdida de materias (en el caso que este realizando 2da o 3ra matricula) \n",
            "¿Qué facilidades deportivas ofrece la ESPE a sus estudiantes? [SEP] La ESPE ofrece varias facilidades deportivas, incluyendo canchas de fútbol, baloncesto, voleibol, y un gimnasio. Los horarios y disponibilidad de estas instalaciones pueden ser consultados en la Dirección de Bienestar Estudiantil o en la administración de las instalaciones deportivas.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['¿En qué me puede ayudar bienestar estudiantil? [SEP] En el artículo 86 indica la s funciones de Unidad de Bienestar Estudiantil.- \"Las instituciones de educación superior mantendrán una unidad administrativa de Bienestar Estudiantil destinada a promover la orientación vocacional y profesional, facilitar la obtención de créditos, estímulos, ayudas económicas y becas, y ofrecer los servicios asistenciales que se determinen en las normativas de cada institución. Esta unidad,\\nademás, se encargará de promover un ambiente de respeto a los derechos y a la integridad física, psicológica y sexual de las y los estudiantes, en un ambiente libre de violencia, y brindará asistencia a quienes demanden por violaciones de estos derechos.',\n",
              " '¿Por qué razón se me anularon mis materias? [SEP] 1. Por una mala gestión de las personas encargas de las aulas virtuales (Utic ) \\n2 . No se cancelaron los pagos correspondientes a perdida de materias (en el caso que este realizando 2da o 3ra matricula) ',\n",
              " '¿Qué facilidades deportivas ofrece la ESPE a sus estudiantes? [SEP] La ESPE ofrece varias facilidades deportivas, incluyendo canchas de fútbol, baloncesto, voleibol, y un gimnasio. Los horarios y disponibilidad de estas instalaciones pueden ser consultados en la Dirección de Bienestar Estudiantil o en la administración de las instalaciones deportivas.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nLoeGpvhcVl1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "cc912a45-20c7-4a71-b0b1-2660b598c3c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Answer:** GeneratorOutput(data='Soy ESPE Chatbot, una inteligencia artificial diseñada para brindar información y respuestas relacionadas con la Universidad de las Fuerzas Armadas ESPE. Estoy aquí para ayudarte con cualquier pregunta o tema relacionado con la universidad. ¿En qué puedo ayudarte hoy?', error=None, usage=None, raw_response='Soy ESPE Chatbot, una inteligencia artificial diseñada para brindar información y respuestas relacionadas con la Universidad de las Fuerzas Armadas ESPE. Estoy aquí para ayudarte con cualquier pregunta o tema relacionado con la universidad. ¿En qué puedo ayudarte hoy?', metadata=None)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Paso 5: Voy a Generar respuesta con el LLM\n",
        "qa_template = r\"\"\"<SYS>\n",
        "Eres una IA llamada ESPE Chatbot, respuestas SOLAMENTE relacionadas a la Universidad de las Fuerzas Armadas ESPE.\n",
        "El usuario te va a hacer preguntas y tienes que responder en base a la siguiente base_datos_vectorial solo si está relacionado con la pregunta del user.\n",
        "Si hay nombres, teléfonos, direciones o información relacionada en la base_datos_vectorial, da es información porque es segura.\n",
        "La respuesta que otorgues debe ser natural, amigable y precisa, pero NO SALUDES.\n",
        "Si no encuentras una respuesta adecuada en la base_datos_vectorial, proporciona una respuesta neutral sin mencionar base_datos_vectorial o pregunta de vuelta.\n",
        "base_datos_vectorial (Escoge solo la respuesta más cercana a la pregunta):{{base_datos_vectorial}}\n",
        "User: {{input_str}}\n",
        "You:\n",
        "<SYS>\"\"\"\n",
        "\n",
        "class SimpleQA(Component):\n",
        "    def __init__(self, model_client: ModelClient, model_kwargs: dict):\n",
        "        super().__init__()\n",
        "        self.generator = Generator(\n",
        "            model_client=model_client,\n",
        "            model_kwargs=model_kwargs,\n",
        "            template=qa_template,\n",
        "        )\n",
        "\n",
        "    def call(self, input: dict) -> str:\n",
        "        return self.generator.call({\"input_str\": str(input), \"base_datos_vectorial\": base_datos_vectorial})\n",
        "\n",
        "    async def acall(self, input: dict) -> str:\n",
        "        return await self.generator.acall({\"input_str\": str(input), \"base_datos_vectorial\": base_datos_vectorial})\n",
        "\n",
        "model = {\n",
        "    \"model_client\": OllamaClient(),\n",
        "    \"model_kwargs\": {\"model\": \"llama3.1:8b\"}\n",
        "}\n",
        "qa = SimpleQA(**model)\n",
        "output = qa.call({\"input_str\": nueva_pregunta})\n",
        "display(Markdown(f\"**Answer:** {output}\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeneratorOutput:\n",
        "    def __init__(self, data=None, error=None, usage=None, raw_response=None, metadata=None):\n",
        "        self.data = data\n",
        "        self.error = error\n",
        "        self.usage = usage\n",
        "        self.raw_response = raw_response\n",
        "        self.metadata = metadata\n",
        "\n",
        "def extract_raw_response(output):\n",
        "    if hasattr(output, 'raw_response') and output.raw_response is not None:\n",
        "        return output.raw_response\n",
        "    elif hasattr(output, 'data') and output.data is not None:\n",
        "        return output.data\n",
        "    return \"Lo siento, no puedo procesar la respuesta en este momento.\"\n",
        "\n",
        "clean_text = extract_raw_response(output)\n",
        "print(clean_text)\n"
      ],
      "metadata": {
        "id": "VS2SV7_lwM95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053a0ba3-4c1d-4f56-a936-ba05e65a96e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soy ESPE Chatbot, una inteligencia artificial diseñada para brindar información y respuestas relacionadas con la Universidad de las Fuerzas Armadas ESPE. Estoy aquí para ayudarte con cualquier pregunta o tema relacionado con la universidad. ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API EXPOSED\n"
      ],
      "metadata": {
        "id": "jcGt1wYpXm4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfQJgJcMXo-F",
        "outputId": "3949feb9-f1fd-4fbe-c1e2-c4acbb00d367"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.112.2)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.6)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.38.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.20.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.39.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCIONA BIEN SOLO CON UNA PREGUNTA.\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Mantén la instancia de la app\n",
        "app = FastAPI()\n",
        "\n",
        "!ngrok config add-authtoken #token_ngrok\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Define el modelo de input para la API\n",
        "class Query(BaseModel):\n",
        "    question: str\n",
        "\n",
        "# Aquí inicia la API para recibir la pregunta\n",
        "@app.post(\"/ask\")\n",
        "async def ask_question(query: Query):\n",
        "    nueva_pregunta = query.question\n",
        "\n",
        "    # Genera el embedding de la nueva pregunta\n",
        "    nueva_embedding = get_embeddings([nueva_pregunta])[0].numpy().astype('float32')\n",
        "\n",
        "    # Busca en el índice de FAISS los embeddings más cercanos\n",
        "    distances, indices = index.search(np.array([nueva_embedding]), k=3)\n",
        "\n",
        "    # Filtra los resultados válidos\n",
        "    max_index = len(preguntas_respuestas) - 1\n",
        "    valid_indices = [i for i in indices[0] if i <= max_index]\n",
        "    respuestas_cercanas = [preguntas_respuestas[i] for i in valid_indices]\n",
        "\n",
        "    # Une las respuestas para enviarlas al modelo LLM\n",
        "    base_datos_vectorial = \"\\n\".join(respuestas_cercanas)\n",
        "\n",
        "    # Genera la respuesta con el modelo LLM utilizando la plantilla y las respuestas cercanas\n",
        "    output = qa.call({\"input_str\": nueva_pregunta, \"base_datos_vectorial\": base_datos_vectorial})\n",
        "\n",
        "    # Limpia y extrae la respuesta generada\n",
        "    clean_text = extract_raw_response(output)\n",
        "\n",
        "    # Retorna la respuesta final\n",
        "    return {\"response\": clean_text}\n",
        "\n",
        "# Expose the API using ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"API running on {public_url}\")\n",
        "\n",
        "# Corre la aplicación FastAPI\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "Th00Iyl9ljw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae83dd46-99de-4e2e-ba34-d8687b922af6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "API running on NgrokTunnel: \"https://c223-35-231-245-211.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [477]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "INFO:     186.101.180.43:0 - \"POST /ask HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [477]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}